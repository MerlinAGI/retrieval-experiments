{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from typing import List,Tuple\n",
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import llm\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_FILE = \"data/input/MSA_Juniper_IBM.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read text and chunk it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 45290\n",
      "Number of chunks: 400: 865, 1000: 393, 4000: 93\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def token_count(text):\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "with open(TEXT_FILE) as f:\n",
    "    all_text = f.read()\n",
    "    # replace non-asci characters\n",
    "    all_text = all_text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "print(f\"Total tokens: {token_count(all_text)}\")\n",
    "\n",
    "\n",
    "def split_text(text, chunk_size=400):\n",
    "    overlap = chunk_size // 4\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "chunks_400 = split_text(all_text, chunk_size=400)\n",
    "chunks_1000 = split_text(all_text, chunk_size=1000)\n",
    "chunks_4000 = split_text(all_text, chunk_size=4000)\n",
    "chunks = chunks_400 + chunks_1000 + chunks_4000\n",
    "\n",
    "print(f\"Number of chunks: 400: {len(chunks_400)}, 1000: {len(chunks_1000)}, 4000: {len(chunks_4000)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_ada(texts:List[str]):\n",
    "    model = \"text-embedding-ada-002\"\n",
    "    data = openai.Embedding.create(input = texts, model=model)['data']\n",
    "    return np.array([d['embedding'] for d in data])\n",
    "\n",
    "def load_embeddings_ada(texts:List[str]):\n",
    "    emb_file = \"data/embeddings/ada.npy\"\n",
    "    if os.path.exists(emb_file):\n",
    "        return np.load(emb_file)\n",
    "    embs = get_embedding_ada(texts)\n",
    "    os.makedirs(os.path.dirname(emb_file), exist_ok=True)\n",
    "    np.save(emb_file, embs)\n",
    "    return embs\n",
    "\n",
    "embs_ada = load_embeddings_ada(chunks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_functions = {\n",
    "    \"ada\": get_embedding_ada,\n",
    "}\n",
    "embeddings = {\n",
    "    \"ada\": embs_ada,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyDe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothetical_answers(question, n=3, model=\"turbo\", prompt_key=\"hyde_general\"):\n",
    "    with open(f\"prompts/{prompt_key}.txt\") as f:\n",
    "        prompt = f.read()\n",
    "    answers = llm.complete(prompt, question, model=model, temp=0.7, max_tokens=150, n=n, return_list=True)\n",
    "    # save answers to file\n",
    "    question_name = question.replace(\" \", \"_\").replace(\"?\", \"\")[:20]\n",
    "    folder = f\"output/hyde/{question_name}\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    with open(f\"{folder}/{prompt_key}_{model}.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\\n\\n####\\n\\n\\n\".join(answers))\n",
    "    return answers\n",
    "\n",
    "def avg_embedding(answers, emb_type=\"ada\"):\n",
    "    answ_embs = emb_functions[emb_type](answers)\n",
    "    avg = np.mean(answ_embs, axis=0)\n",
    "    # print the mean cosine similarity to the average\n",
    "    closeness = np.average([np.dot(avg, emb) for emb in answ_embs])\n",
    "    print(f\"Similarity within hypotheticals: {closeness:.3f}\")\n",
    "    return avg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_cos(text, emb_type=\"ada\", top_k=1000, text_embedding=None) -> List[Tuple[str, float]]:\n",
    "    if text_embedding is None:\n",
    "        text_embedding = emb_functions[emb_type]([text])[0]\n",
    "    embs = embeddings[emb_type]\n",
    "    \n",
    "    # find argmax of cosine similarity. All vectors are normalized, so this is equivalent to argmax of dot product\n",
    "    cos_sim = np.dot(embs, text_embedding)\n",
    "    top_k_indices = np.argsort(cos_sim)[-top_k:]\n",
    "    top_k_indices = top_k_indices[::-1]\n",
    "    return [(chunks[i], cos_sim[i]) for i in top_k_indices]\n",
    "\n",
    "def print_context(context: List[Tuple[str, float]]):\n",
    "    for i, (chunk, cos) in enumerate(context):\n",
    "        print(\"-\"*30 + f\" Result {i}: {cos:.3f} \" + \"-\"*30)\n",
    "        print(chunk)\n",
    "\n",
    "def answer_pos_in_context(context: List[Tuple[str, float]], answer)-> Tuple[int, float]:\n",
    "    for i, (chunk, cos) in enumerate(context):\n",
    "        if answer in chunk:\n",
    "            return i, cos\n",
    "    return None, None\n",
    "\n",
    "def eval_context(context, answer, do_print_context=False):\n",
    "    #context = get_context_cos(question, emb_type=emb_type, top_k=top_k)\n",
    "    top_score = context[0][1]\n",
    "    answer_pos, answer_score = answer_pos_in_context(context, answer)\n",
    "    print(f\"Answer pos: {answer_pos}. Answer score: {answer_score:.3f}. First result score: {top_score:.3f}\")\n",
    "    if do_print_context:\n",
    "        print_context(context)\n",
    "    return answer_pos\n",
    "\n",
    "def run_test(question, answer, emb_type=\"ada\", top_k=1000, do_print_context=False):\n",
    "    context = get_context_cos(question, emb_type=emb_type, top_k=top_k)\n",
    "    answer_pos = eval_context(context, answer, do_print_context=do_print_context)\n",
    "    return answer_pos\n",
    "\n",
    "def run_test_hyde(question, answer, emb_type=\"ada\", model=\"turbo\", prompt_key=\"hyde_general\", top_k=1000, do_print_context=False):\n",
    "    hyde_answer_emb = avg_embedding(hypothetical_answers(question, model=model, prompt_key=prompt_key), emb_type=emb_type)\n",
    "    context = get_context_cos(question, emb_type=emb_type, top_k=top_k, text_embedding=hyde_answer_emb)\n",
    "    answer_pos = eval_context(context, answer, do_print_context=do_print_context)\n",
    "    return answer_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer pos: 1. Answer score: 0.803. First result score: 0.807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_test(\"How often do we have access to training by IBM?\", \"once every Contract Year or upon request after at least thirty(30) days\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running baseline_ada on 1/7\n",
      "Answer pos: 178. Answer score: 0.758. First result score: 0.794\n",
      "Running baseline_ada on 2/7\n",
      "Answer pos: 8. Answer score: 0.787. First result score: 0.804\n",
      "Running baseline_ada on 3/7\n",
      "Answer pos: 235. Answer score: 0.800. First result score: 0.849\n",
      "Running baseline_ada on 4/7\n",
      "Answer pos: 11. Answer score: 0.792. First result score: 0.831\n",
      "Running baseline_ada on 5/7\n",
      "Answer pos: 2. Answer score: 0.798. First result score: 0.801\n",
      "Running baseline_ada on 6/7\n",
      "Answer pos: 14. Answer score: 0.788. First result score: 0.809\n",
      "Running baseline_ada on 7/7\n",
      "Answer pos: 10. Answer score: 0.797. First result score: 0.813\n",
      "Running hyde_general_turbo_ada on 1/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.943\n",
      "Answer pos: 5. Answer score: 0.819. First result score: 0.830\n",
      "Running hyde_general_turbo_ada on 2/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.951\n",
      "Answer pos: 67. Answer score: 0.800. First result score: 0.839\n",
      "Running hyde_general_turbo_ada on 3/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.978\n",
      "Answer pos: 80. Answer score: 0.826. First result score: 0.862\n",
      "Running hyde_general_turbo_ada on 4/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.943\n",
      "Answer pos: 9. Answer score: 0.836. First result score: 0.863\n",
      "Running hyde_general_turbo_ada on 5/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.969\n",
      "Answer pos: 6. Answer score: 0.817. First result score: 0.826\n",
      "Running hyde_general_turbo_ada on 6/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.949\n",
      "Answer pos: 1. Answer score: 0.831. First result score: 0.837\n",
      "Running hyde_general_turbo_ada on 7/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.978\n",
      "Answer pos: 19. Answer score: 0.835. First result score: 0.856\n",
      "Running hyde_one_shot_turbo_ada on 1/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.971\n",
      "Answer pos: 114. Answer score: 0.839. First result score: 0.905\n",
      "Running hyde_one_shot_turbo_ada on 2/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.937\n",
      "Answer pos: 24. Answer score: 0.844. First result score: 0.878\n",
      "Running hyde_one_shot_turbo_ada on 3/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.952\n",
      "Answer pos: 21. Answer score: 0.858. First result score: 0.885\n",
      "Running hyde_one_shot_turbo_ada on 4/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.969\n",
      "Answer pos: 15. Answer score: 0.870. First result score: 0.890\n",
      "Running hyde_one_shot_turbo_ada on 5/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.949\n",
      "Answer pos: 5. Answer score: 0.837. First result score: 0.848\n",
      "Running hyde_one_shot_turbo_ada on 6/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.957\n",
      "Answer pos: 0. Answer score: 0.895. First result score: 0.895\n",
      "Running hyde_one_shot_turbo_ada on 7/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.981\n",
      "Answer pos: 8. Answer score: 0.856. First result score: 0.873\n",
      "Running hyde_general_gpt-4_ada on 1/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.957\n",
      "Answer pos: 3. Answer score: 0.827. First result score: 0.838\n",
      "Running hyde_general_gpt-4_ada on 2/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.939\n",
      "Answer pos: 26. Answer score: 0.835. First result score: 0.856\n",
      "Running hyde_general_gpt-4_ada on 3/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.978\n",
      "Answer pos: 85. Answer score: 0.828. First result score: 0.875\n",
      "Running hyde_general_gpt-4_ada on 4/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.953\n",
      "Answer pos: 4. Answer score: 0.864. First result score: 0.865\n",
      "Running hyde_general_gpt-4_ada on 5/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.964\n",
      "Answer pos: 2. Answer score: 0.829. First result score: 0.832\n",
      "Running hyde_general_gpt-4_ada on 6/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.943\n",
      "Answer pos: 9. Answer score: 0.864. First result score: 0.885\n",
      "Running hyde_general_gpt-4_ada on 7/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.945\n",
      "Answer pos: 0. Answer score: 0.866. First result score: 0.866\n",
      "Running hyde_one_shot_gpt-4_ada on 1/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.958\n",
      "Answer pos: 23. Answer score: 0.842. First result score: 0.887\n",
      "Running hyde_one_shot_gpt-4_ada on 2/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.949\n",
      "Answer pos: 0. Answer score: 0.875. First result score: 0.875\n",
      "Running hyde_one_shot_gpt-4_ada on 3/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.980\n",
      "Answer pos: 10. Answer score: 0.870. First result score: 0.891\n",
      "Running hyde_one_shot_gpt-4_ada on 4/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.972\n",
      "Answer pos: 17. Answer score: 0.864. First result score: 0.886\n",
      "Running hyde_one_shot_gpt-4_ada on 5/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.963\n",
      "Answer pos: 0. Answer score: 0.865. First result score: 0.865\n",
      "Running hyde_one_shot_gpt-4_ada on 6/7\n",
      "running completion\n",
      "Similarity within hypotheticals: 0.975\n",
      "Answer pos: 2. Answer score: 0.893. First result score: 0.899\n",
      "Running hyde_one_shot_gpt-4_ada on 7/7\n",
      "running completion\n",
      "Similarity within hypotheticals: 0.948\n",
      "Answer pos: 3. Answer score: 0.876. First result score: 0.889\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/benchmark_qa.json\") as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "embedding_types = [\"ada\"]\n",
    "\n",
    "ranks = {}\n",
    "\n",
    "def run_tests(test_func, test_name):\n",
    "    # Baseline (no hyde)\n",
    "    for emb_type in embedding_types:\n",
    "        name = f\"{test_name}_{emb_type}\"\n",
    "        ranks[name] = []\n",
    "        for i, qa in enumerate(qa_data):\n",
    "            print(f\"Running {name} on {i+1}/{len(qa_data)}\")\n",
    "            answer_pos = test_func(qa[\"q\"], qa[\"a\"], emb_type=emb_type)\n",
    "            ranks[name].append(answer_pos)\n",
    "\n",
    "run_tests(run_test, \"baseline\")\n",
    "\n",
    "# Hyde tests\n",
    "models = [\"turbo\", \"gpt-4\"]\n",
    "prompt_keys = [\"hyde_general\", \"hyde_one_shot\"]\n",
    "\n",
    "for model in models:\n",
    "    for prompt_key in prompt_keys:\n",
    "        name = f\"{prompt_key}_{model}\"\n",
    "        func = lambda q, a, emb_type=\"ada\": run_test_hyde(q, a, emb_type=emb_type, model=model, prompt_key=prompt_key)\n",
    "        run_tests(func, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  baseline_ada  \\\n",
      "0  Will IBM help us getting started? Will they su...           178   \n",
      "1     If we get aquired, do we have to stick to IBM?             8   \n",
      "2  Is IBM allowed to subcontract any of its oblig...           235   \n",
      "3            Are they allowed to use subcontractors?            11   \n",
      "4  Can IBM change the price? Under which conditions?             2   \n",
      "5  Are they allowed to share our data with third ...            14   \n",
      "6  What happens if IBM get hacked and customer da...            10   \n",
      "\n",
      "   hyde_general_turbo_ada  hyde_one_shot_turbo_ada  hyde_general_gpt-4_ada  \\\n",
      "0                       5                      114                       3   \n",
      "1                      67                       24                      26   \n",
      "2                      80                       21                      85   \n",
      "3                       9                       15                       4   \n",
      "4                       6                        5                       2   \n",
      "5                       1                        0                       9   \n",
      "6                      19                        8                       0   \n",
      "\n",
      "   hyde_one_shot_gpt-4_ada  \n",
      "0                       23  \n",
      "1                        0  \n",
      "2                       10  \n",
      "3                       17  \n",
      "4                        0  \n",
      "5                        2  \n",
      "6                        3  \n"
     ]
    }
   ],
   "source": [
    "def print_results(ranks):\n",
    "    # make table, rows is questions, columns is \"question\", and then all methods methods\n",
    "    df = pd.DataFrame(columns=[\"question\"] + list(ranks.keys()))\n",
    "    for i, qa in enumerate(qa_data):\n",
    "        row = [qa[\"q\"]]\n",
    "        for name, rank in ranks.items():\n",
    "            row.append(rank[i])\n",
    "        df.loc[i] = row\n",
    "    print(df)\n",
    "    return df\n",
    "df = print_results(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_benchmark(svm=False):\n",
    "    # Set the width of a bar\n",
    "    bar_width = 0.15\n",
    "\n",
    "    # Set the positions of bars on x-axis\n",
    "    ind = list(range(len(df)))\n",
    "\n",
    "    \"\"\"\n",
    "    pos_plain = [x - bar_width for x in ind]\n",
    "    pos_hyde = [x for x in ind]\n",
    "    pos_instruct = [x + bar_width for x in ind]\n",
    "    pos_instruct_hyde = [x + 2 * bar_width for x in ind]\n",
    "\n",
    "    # Create the subplots for each group\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the bars\n",
    "    svm_t = \"svm_\" if svm else \"\"\n",
    "    ax.bar(pos_plain, df[f\"plain_{svm_t}plain\"], bar_width, label=\"plain\")\n",
    "    ax.bar(pos_hyde, df[f\"hyde_{svm_t}plain\"], bar_width, label=\"hyde\")\n",
    "    ax.bar(pos_instruct, df[f\"plain_{svm_t}instruct\"], bar_width, label=\"instruct\")\n",
    "    ax.bar(pos_instruct_hyde, df[f\"hyde_{svm_t}instruct\"], bar_width, label=\"hyde + instruct\")\n",
    "\n",
    "    # Set the x-axis ticks and labels\n",
    "    ax.set_xticks([x + bar_width for x in range(len(df))])\n",
    "    labels = [f'{i}: {q[\"q\"][:10]}' for i,q in enumerate(questions)]\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    \"\"\"\n",
    "\n",
    "    # make a similar plot\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "   \n",
    "\n",
    "plot_benchmark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
