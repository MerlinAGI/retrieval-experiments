{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from typing import List,Tuple\n",
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import llm\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_FILE = \"data/input/MSA_Juniper_IBM.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read text and chunk it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 45290\n",
      "Number of chunks: 400: 865, 1000: 393, 4000: 93\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def token_count(text):\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "with open(TEXT_FILE) as f:\n",
    "    all_text = f.read()\n",
    "    # replace non-asci characters\n",
    "    all_text = all_text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "print(f\"Total tokens: {token_count(all_text)}\")\n",
    "\n",
    "\n",
    "def split_text(text, chunk_size=400):\n",
    "    overlap = chunk_size // 4\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "chunks_400 = split_text(all_text, chunk_size=400)\n",
    "chunks_1000 = split_text(all_text, chunk_size=1000)\n",
    "chunks_4000 = split_text(all_text, chunk_size=4000)\n",
    "chunks = chunks_400 + chunks_1000 + chunks_4000\n",
    "\n",
    "print(f\"Number of chunks: 400: {len(chunks_400)}, 1000: {len(chunks_1000)}, 4000: {len(chunks_4000)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_ada(texts:List[str]):\n",
    "    model = \"text-embedding-ada-002\"\n",
    "    data = openai.Embedding.create(input = texts, model=model)['data']\n",
    "    return np.array([d['embedding'] for d in data])\n",
    "\n",
    "def load_embeddings_ada(texts:List[str]):\n",
    "    emb_file = \"data/embeddings/ada.npy\"\n",
    "    if os.path.exists(emb_file):\n",
    "        return np.load(emb_file)\n",
    "    embs = get_embedding_ada(texts)\n",
    "    os.makedirs(os.path.dirname(emb_file), exist_ok=True)\n",
    "    np.save(emb_file, embs)\n",
    "    return embs\n",
    "\n",
    "embs_ada = load_embeddings_ada(chunks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_functions = {\n",
    "    \"ada\": get_embedding_ada,\n",
    "}\n",
    "embeddings = {\n",
    "    \"ada\": embs_ada,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyDe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothetical_answers(question, n=3):\n",
    "    prompt = \"Write a passage of a service agreement contract between Juniper (we, \\\"Customer\\\") and IBM (them, \\\"Provider\\\") that answers the user's question. Do not use the names but only use Customer and Provider.\"\n",
    "    return llm.complete(prompt, question, model=\"turbo\", temp=0.7, max_tokens=200, n=n, return_list=True)\n",
    "\n",
    "def avg_embedding(answers, emb_type=\"ada\"):\n",
    "    answ_embs = emb_functions[emb_type](answers)\n",
    "    avg = np.mean(answ_embs, axis=0)\n",
    "    # print the mean cosine similarity to the average\n",
    "    closeness = np.average([np.dot(avg, emb) for emb in answ_embs])\n",
    "    print(f\"Similarity within hypotheticals: {closeness:.3f}\")\n",
    "    return avg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_cos(text, emb_type=\"ada\", top_k=1000, text_embedding=None) -> List[Tuple[str, float]]:\n",
    "    if text_embedding is None:\n",
    "        text_embedding = emb_functions[emb_type]([text])[0]\n",
    "    embs = embeddings[emb_type]\n",
    "    \n",
    "    # find argmax of cosine similarity. All vectors are normalized, so this is equivalent to argmax of dot product\n",
    "    cos_sim = np.dot(embs, text_embedding)\n",
    "    top_k_indices = np.argsort(cos_sim)[-top_k:]\n",
    "    top_k_indices = top_k_indices[::-1]\n",
    "    return [(chunks[i], cos_sim[i]) for i in top_k_indices]\n",
    "\n",
    "def print_context(context: List[Tuple[str, float]]):\n",
    "    for i, (chunk, cos) in enumerate(context):\n",
    "        print(\"-\"*30 + f\" Result {i}: {cos:.3f} \" + \"-\"*30)\n",
    "        print(chunk)\n",
    "\n",
    "def answer_pos_in_context(context: List[Tuple[str, float]], answer)-> Tuple[int, float]:\n",
    "    for i, (chunk, cos) in enumerate(context):\n",
    "        if answer in chunk:\n",
    "            return i, cos\n",
    "    return None, None\n",
    "\n",
    "def eval_context(context, answer, do_print_context=False):\n",
    "    #context = get_context_cos(question, emb_type=emb_type, top_k=top_k)\n",
    "    top_score = context[0][1]\n",
    "    answer_pos, answer_score = answer_pos_in_context(context, answer)\n",
    "    print(f\"Answer pos: {answer_pos}. Answer score: {answer_score:.3f}. First result score: {top_score:.3f}\")\n",
    "    if do_print_context:\n",
    "        print_context(context)\n",
    "    return answer_pos\n",
    "\n",
    "def run_test(question, answer, emb_type=\"ada\", top_k=1000, do_print_context=False):\n",
    "    context = get_context_cos(question, emb_type=emb_type, top_k=top_k)\n",
    "    answer_pos = eval_context(context, answer, do_print_context=do_print_context)\n",
    "    return answer_pos\n",
    "\n",
    "def run_test_hyde(question, answer, emb_type=\"ada\", top_k=1000, do_print_context=False):\n",
    "    hyde_answer_emb = avg_embedding(hypothetical_answers(question), emb_type=emb_type)\n",
    "    context = get_context_cos(question, emb_type=emb_type, top_k=top_k, text_embedding=hyde_answer_emb)\n",
    "    answer_pos = eval_context(context, answer, do_print_context=do_print_context)\n",
    "    return answer_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer pos: 1. Answer score: 0.802. First result score: 0.807\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.969\n",
      "Answer pos: 2. Answer score: 0.894. First result score: 0.900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_test(\"How often do we have access to training by IBM?\", \"once every Contract Year or upon request after at least thirty(30) days\")\n",
    "run_test_hyde(\"How often do we have access to training by IBM?\", \"once every Contract Year or upon request after at least thirty(30) days\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running plain_ada on 0/7\n",
      "Answer pos: 178. Answer score: 0.758. First result score: 0.794\n",
      "Running plain_ada on 1/7\n",
      "Answer pos: 8. Answer score: 0.787. First result score: 0.804\n",
      "Running plain_ada on 2/7\n",
      "Answer pos: 235. Answer score: 0.800. First result score: 0.849\n",
      "Running plain_ada on 3/7\n",
      "Answer pos: 11. Answer score: 0.792. First result score: 0.831\n",
      "Running plain_ada on 4/7\n",
      "Answer pos: 2. Answer score: 0.798. First result score: 0.801\n",
      "Running plain_ada on 5/7\n",
      "Answer pos: 14. Answer score: 0.788. First result score: 0.809\n",
      "Running plain_ada on 6/7\n",
      "Answer pos: 10. Answer score: 0.797. First result score: 0.812\n",
      "Running hyde_ada on 0/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.957\n",
      "Answer pos: 1. Answer score: 0.879. First result score: 0.884\n",
      "Running hyde_ada on 1/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.954\n",
      "Answer pos: 6. Answer score: 0.879. First result score: 0.892\n",
      "Running hyde_ada on 2/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.982\n",
      "Answer pos: 13. Answer score: 0.874. First result score: 0.890\n",
      "Running hyde_ada on 3/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.976\n",
      "Answer pos: 7. Answer score: 0.893. First result score: 0.909\n",
      "Running hyde_ada on 4/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.958\n",
      "Answer pos: 28. Answer score: 0.861. First result score: 0.891\n",
      "Running hyde_ada on 5/7\n",
      "using cached result\n",
      "Similarity within hypotheticals: 0.976\n",
      "Answer pos: 1. Answer score: 0.923. First result score: 0.927\n",
      "Running hyde_ada on 6/7\n",
      "running completion\n",
      "Similarity within hypotheticals: 0.971\n",
      "Answer pos: 4. Answer score: 0.891. First result score: 0.898\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/benchmark_qa.json\") as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "embedding_types = [\"ada\"]\n",
    "approaches = {\n",
    "    \"plain\": run_test,\n",
    "    \"hyde\": run_test_hyde,\n",
    "}\n",
    "\n",
    "ranks = {}\n",
    "\n",
    "for emb_type in embedding_types:\n",
    "    for approach_name, approach in approaches.items():\n",
    "        name = f\"{approach_name}_{emb_type}\"\n",
    "        ranks[name] = []\n",
    "        for i, qa in enumerate(qa_data):\n",
    "            print(f\"Running {name} on {i}/{len(qa_data)}\")\n",
    "            answer_pos = approach(qa[\"q\"], qa[\"a\"])\n",
    "            ranks[name].append(answer_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  plain_ada  hyde_ada\n",
      "0  Will IBM help us getting started? Will they su...        178         1\n",
      "1     If we get aquired, do we have to stick to IBM?          8         6\n",
      "2  Is IBM allowed to subcontract any of its oblig...        235        13\n",
      "3            Are they allowed to use subcontractors?         11         7\n",
      "4  Can IBM change the price? Under which conditions?          2        28\n",
      "5  Are they allowed to share our data with third ...         14         1\n",
      "6  What happens if IBM get hacked and customer da...         10         4\n"
     ]
    }
   ],
   "source": [
    "def print_results(ranks):\n",
    "    # make table, rows is questions, columns is \"question\", and then all methods methods\n",
    "    df = pd.DataFrame(columns=[\"question\"] + list(ranks.keys()))\n",
    "    for i, qa in enumerate(qa_data):\n",
    "        row = [qa[\"q\"]]\n",
    "        for name, rank in ranks.items():\n",
    "            row.append(rank[i])\n",
    "        df.loc[i] = row\n",
    "    print(df)\n",
    "    return df\n",
    "df = print_results(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_benchmark(svm=False):\n",
    "    # Set the width of a bar\n",
    "    bar_width = 0.15\n",
    "\n",
    "    # Set the positions of bars on x-axis\n",
    "    ind = list(range(len(df)))\n",
    "\n",
    "    \"\"\"\n",
    "    pos_plain = [x - bar_width for x in ind]\n",
    "    pos_hyde = [x for x in ind]\n",
    "    pos_instruct = [x + bar_width for x in ind]\n",
    "    pos_instruct_hyde = [x + 2 * bar_width for x in ind]\n",
    "\n",
    "    # Create the subplots for each group\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the bars\n",
    "    svm_t = \"svm_\" if svm else \"\"\n",
    "    ax.bar(pos_plain, df[f\"plain_{svm_t}plain\"], bar_width, label=\"plain\")\n",
    "    ax.bar(pos_hyde, df[f\"hyde_{svm_t}plain\"], bar_width, label=\"hyde\")\n",
    "    ax.bar(pos_instruct, df[f\"plain_{svm_t}instruct\"], bar_width, label=\"instruct\")\n",
    "    ax.bar(pos_instruct_hyde, df[f\"hyde_{svm_t}instruct\"], bar_width, label=\"hyde + instruct\")\n",
    "\n",
    "    # Set the x-axis ticks and labels\n",
    "    ax.set_xticks([x + bar_width for x in range(len(df))])\n",
    "    labels = [f'{i}: {q[\"q\"][:10]}' for i,q in enumerate(questions)]\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    \"\"\"\n",
    "\n",
    "    # make a similar plot\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "   \n",
    "\n",
    "plot_benchmark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
